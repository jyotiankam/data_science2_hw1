---
title: "Data Science II HW1"
author: "Jyoti Ankam"
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
```

```{r}

library(tidyverse)
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)
library(pls)
library(ISLR)

```

Reading/loading the datasets:

```{r}
train_solubility <- read.csv("C:/Users/19293/Desktop/Data Science II/solubility_train.csv") 

test_solubility <- read.csv("C:/Users/19293/Desktop/Data Science II/solubility_test.csv")

```

The data has been divided into training and test datasets containing 229 variables each. There are 951 observations in the training dataset and 316 observations in the test dataset.

#Least sqaures
Fitting the linear model using least sqaures on the training data:

```{r}

fit1 <- lm(Solubility ~ . , data = train_solubility)

fit2 <- lm(Solubility ~ . , data = test_solubility)

mean((test_solubility$Solubility - predict.lm(fit1, test_solubility)) ^ 2)

```
The mean square error using the test data is 0.5558

#Ridge regression
Fitting a ridge regression model on the training data, with λ chosen by cross-validation.
```{r}

X.Training = model.matrix(Solubility ~ ., train_solubility)[,-1]
Y.Training = train_solubility$Solubility

ridge.mod <- glmnet(X.Training, Y.Training, alpha=0, lambda = exp(seq(-10, 10, length=200)))
plot(ridge.mod)

set.seed(1)
cv.ridge <- cv.glmnet(X.Training, Y.Training, 
                      alpha = 0, 
                      lambda = exp(seq(-10, 10, length=200)),
                      type.measure = "mse")

plot(cv.ridge)

optimal_lamda = cv.ridge$lambda.min
```

Using the lambda chosen by cross validation:
```{r}
X.Testing = model.matrix(Solubility ~ ., test_solubility)[,-1]
Y.Testing = test_solubility$Solubility

set.seed(1)
pred = predict(ridge.mod, s = optimal_lamda, newx = X.Testing)
mean((pred - Y.Testing)^2)

```
The mean test error is 0.5128

#Lasso
Fitting a lasso model on the training data, with λ chosen by cross-validation
```{r}
X.Train = model.matrix(Solubility ~ ., train_solubility)[,-1]
Y.Train = train_solubility$Solubility

lasso.mod <- glmnet(X.Train, Y.Train, alpha=1, lambda = exp(seq(-10, 10, length=200)))
plot(lasso.mod)

set.seed(1)
cv.lasso <- cv.glmnet(X.Train, Y.Train, 
                      alpha = 1, 
                      lambda = exp(seq(-10, 10, length=200)),
                      type.measure = "mse")

plot(cv.ridge)

optimum_lamda = cv.lasso$lambda.min

```

```{r}
X.Test = model.matrix(Solubility ~ ., test_solubility)[,-1]
Y.Test = test_solubility$Solubility

set.seed(1)
pred2 = predict(lasso.mod, s = optimum_lamda, newx = X.Test)
mean((pred2 - Y.Test)^2)

```

The test error is 0.4988.

```{r}
lasso_coef = predict(cv.lasso, s="lambda.min", type="coefficients")
length(lasso_coef[lasso_coef != 0])
```
There are 144 non-zero coefficient estimates

### PCR 

Fitting a PCR model on the training data, with M chosen by cross-validation.

```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(2)
pcr.fit <- train(X.Training,Y.Training,
                 method = "pcr",
                 tuneLength = 228,
                 trControl = ctrl1,
                 scale = TRUE)

predy2.pcr2 <- predict(pcr.fit$finalModel, newdata = X.Testing, 
                       ncomp = pcr.fit$bestTune$ncomp)
mean((predy2.pcr2-Y.Testing)^2)
ggplot(pcr.fit, highlight = TRUE) + theme_bw()

```
The test error is 0.54055 and is lowest at M = 149

